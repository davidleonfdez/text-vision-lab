{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit face images using text.**\n",
    "\n",
    "Update a StyleGAN 2 latent code guided by CLIP, aiming to minimize the cosine similarity between the CLIP embedding of the input text and the CLIP embedding of the image synthesized from the trained latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:28:57.241433Z",
     "iopub.status.busy": "2022-05-29T19:28:57.241117Z",
     "iopub.status.idle": "2022-05-29T19:28:59.242768Z",
     "shell.execute_reply": "2022-05-29T19:28:59.242012Z",
     "shell.execute_reply.started": "2022-05-29T19:28:57.241349Z"
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import IPython\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:28:59.244901Z",
     "iopub.status.busy": "2022-05-29T19:28:59.244636Z",
     "iopub.status.idle": "2022-05-29T19:28:59.249117Z",
     "shell.execute_reply": "2022-05-29T19:28:59.248424Z",
     "shell.execute_reply.started": "2022-05-29T19:28:59.244865Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' #'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the latent code that corresponds to the input image. Projection doesn't work that well, so we'll use a pretrained encoder, developed by the creators of https://github.com/eladrich/pixel2style2pixel. It also contains the decoder, a StyleGAN 2 face generator.\n",
    "\n",
    "The encoding code used in this notebook has been extracted and simplified from https://github.com/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Ninja to load C++ extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:29:00.523872Z",
     "iopub.status.busy": "2022-05-29T19:29:00.523302Z",
     "iopub.status.idle": "2022-05-29T19:29:02.941499Z",
     "shell.execute_reply": "2022-05-29T19:29:02.940689Z",
     "shell.execute_reply.started": "2022-05-29T19:29:00.523831Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download encoder code and add to Python path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-29T19:29:02.944018Z",
     "iopub.status.busy": "2022-05-29T19:29:02.943586Z",
     "iopub.status.idle": "2022-05-29T19:29:07.469943Z",
     "shell.execute_reply": "2022-05-29T19:29:07.469038Z",
     "shell.execute_reply.started": "2022-05-29T19:29:02.943978Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content')\n",
    "ENCODER_CODE_DIR = 'encoder'\n",
    "!git clone https://github.com/eladrich/pixel2style2pixel.git $ENCODER_CODE_DIR\n",
    "sys.path.append(str(Path(ENCODER_CODE_DIR).resolve()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Modify CUDA-only layers implementation to accept CPU as device (credits to https://github.com/rosinality/stylegan2-pytorch.git):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:32:08.669313Z",
     "iopub.status.busy": "2022-05-27T20:32:08.669048Z",
     "iopub.status.idle": "2022-05-27T20:32:08.677447Z",
     "shell.execute_reply": "2022-05-27T20:32:08.676568Z",
     "shell.execute_reply.started": "2022-05-27T20:32:08.669282Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {ENCODER_CODE_DIR}/models/stylegan2/op/fused_act.py\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_path = os.path.dirname(__file__)\n",
    "    fused = load(\n",
    "        \"fused\",\n",
    "        sources=[\n",
    "            os.path.join(module_path, \"fused_bias_act.cpp\"),\n",
    "            os.path.join(module_path, \"fused_bias_act_kernel.cu\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "class FusedLeakyReLUFunctionBackward(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, grad_output, out, bias, negative_slope, scale):\n",
    "        ctx.save_for_backward(out)\n",
    "        ctx.negative_slope = negative_slope\n",
    "        ctx.scale = scale\n",
    "\n",
    "        empty = grad_output.new_empty(0)\n",
    "\n",
    "        grad_input = fused.fused_bias_act(\n",
    "            grad_output.contiguous(), empty, out, 3, 1, negative_slope, scale\n",
    "        )\n",
    "\n",
    "        dim = [0]\n",
    "\n",
    "        if grad_input.ndim > 2:\n",
    "            dim += list(range(2, grad_input.ndim))\n",
    "\n",
    "        if bias:\n",
    "            grad_bias = grad_input.sum(dim).detach()\n",
    "\n",
    "        else:\n",
    "            grad_bias = empty\n",
    "\n",
    "        return grad_input, grad_bias\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradgrad_input, gradgrad_bias):\n",
    "        out, = ctx.saved_tensors\n",
    "        gradgrad_out = fused.fused_bias_act(\n",
    "            gradgrad_input.contiguous(),\n",
    "            gradgrad_bias,\n",
    "            out,\n",
    "            3,\n",
    "            1,\n",
    "            ctx.negative_slope,\n",
    "            ctx.scale,\n",
    "        )\n",
    "\n",
    "        return gradgrad_out, None, None, None, None\n",
    "\n",
    "\n",
    "class FusedLeakyReLUFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bias, negative_slope, scale):\n",
    "        empty = input.new_empty(0)\n",
    "\n",
    "        ctx.bias = bias is not None\n",
    "\n",
    "        if bias is None:\n",
    "            bias = empty\n",
    "\n",
    "        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n",
    "        ctx.save_for_backward(out)\n",
    "        ctx.negative_slope = negative_slope\n",
    "        ctx.scale = scale\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        out, = ctx.saved_tensors\n",
    "\n",
    "        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n",
    "            grad_output, out, ctx.bias, ctx.negative_slope, ctx.scale\n",
    "        )\n",
    "\n",
    "        if not ctx.bias:\n",
    "            grad_bias = None\n",
    "\n",
    "        return grad_input, grad_bias, None, None\n",
    "\n",
    "\n",
    "class FusedLeakyReLU(nn.Module):\n",
    "    def __init__(self, channel, bias=True, negative_slope=0.2, scale=2 ** 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(channel))\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.negative_slope = negative_slope\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
    "\n",
    "\n",
    "def fused_leaky_relu(input, bias=None, negative_slope=0.2, scale=2 ** 0.5):\n",
    "    if input.device.type == \"cpu\":\n",
    "        if bias is not None:\n",
    "            rest_dim = [1] * (input.ndim - bias.ndim - 1)\n",
    "            return (\n",
    "                F.leaky_relu(\n",
    "                    input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2\n",
    "                )\n",
    "                * scale\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return F.leaky_relu(input, negative_slope=0.2) * scale\n",
    "\n",
    "    else:\n",
    "        return FusedLeakyReLUFunction.apply(\n",
    "            input.contiguous(), bias, negative_slope, scale\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:32:09.939043Z",
     "iopub.status.busy": "2022-05-27T20:32:09.938785Z",
     "iopub.status.idle": "2022-05-27T20:32:09.947972Z",
     "shell.execute_reply": "2022-05-27T20:32:09.947256Z",
     "shell.execute_reply.started": "2022-05-27T20:32:09.939014Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {ENCODER_CODE_DIR}/models/stylegan2/op/upfirdn2d.py\n",
    "from collections import abc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_path = os.path.dirname(__file__)\n",
    "    upfirdn2d_op = load(\n",
    "        \"upfirdn2d\",\n",
    "        sources=[\n",
    "            os.path.join(module_path, \"upfirdn2d.cpp\"),\n",
    "            os.path.join(module_path, \"upfirdn2d_kernel.cu\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "class UpFirDn2dBackward(Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n",
    "    ):\n",
    "\n",
    "        up_x, up_y = up\n",
    "        down_x, down_y = down\n",
    "        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n",
    "\n",
    "        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)\n",
    "\n",
    "        grad_input = upfirdn2d_op.upfirdn2d(\n",
    "            grad_output,\n",
    "            grad_kernel,\n",
    "            down_x,\n",
    "            down_y,\n",
    "            up_x,\n",
    "            up_y,\n",
    "            g_pad_x0,\n",
    "            g_pad_x1,\n",
    "            g_pad_y0,\n",
    "            g_pad_y1,\n",
    "        )\n",
    "        grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])\n",
    "\n",
    "        ctx.save_for_backward(kernel)\n",
    "\n",
    "        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n",
    "\n",
    "        ctx.up_x = up_x\n",
    "        ctx.up_y = up_y\n",
    "        ctx.down_x = down_x\n",
    "        ctx.down_y = down_y\n",
    "        ctx.pad_x0 = pad_x0\n",
    "        ctx.pad_x1 = pad_x1\n",
    "        ctx.pad_y0 = pad_y0\n",
    "        ctx.pad_y1 = pad_y1\n",
    "        ctx.in_size = in_size\n",
    "        ctx.out_size = out_size\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradgrad_input):\n",
    "        kernel, = ctx.saved_tensors\n",
    "\n",
    "        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)\n",
    "\n",
    "        gradgrad_out = upfirdn2d_op.upfirdn2d(\n",
    "            gradgrad_input,\n",
    "            kernel,\n",
    "            ctx.up_x,\n",
    "            ctx.up_y,\n",
    "            ctx.down_x,\n",
    "            ctx.down_y,\n",
    "            ctx.pad_x0,\n",
    "            ctx.pad_x1,\n",
    "            ctx.pad_y0,\n",
    "            ctx.pad_y1,\n",
    "        )\n",
    "        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])\n",
    "        gradgrad_out = gradgrad_out.view(\n",
    "            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]\n",
    "        )\n",
    "\n",
    "        return gradgrad_out, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class UpFirDn2d(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, kernel, up, down, pad):\n",
    "        up_x, up_y = up\n",
    "        down_x, down_y = down\n",
    "        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n",
    "\n",
    "        kernel_h, kernel_w = kernel.shape\n",
    "        batch, channel, in_h, in_w = input.shape\n",
    "        ctx.in_size = input.shape\n",
    "\n",
    "        input = input.reshape(-1, in_h, in_w, 1)\n",
    "\n",
    "        ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))\n",
    "\n",
    "        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h + down_y) // down_y\n",
    "        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w + down_x) // down_x\n",
    "        ctx.out_size = (out_h, out_w)\n",
    "\n",
    "        ctx.up = (up_x, up_y)\n",
    "        ctx.down = (down_x, down_y)\n",
    "        ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)\n",
    "\n",
    "        g_pad_x0 = kernel_w - pad_x0 - 1\n",
    "        g_pad_y0 = kernel_h - pad_y0 - 1\n",
    "        g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1\n",
    "        g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1\n",
    "\n",
    "        ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)\n",
    "\n",
    "        out = upfirdn2d_op.upfirdn2d(\n",
    "            input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n",
    "        )\n",
    "        # out = out.view(major, out_h, out_w, minor)\n",
    "        out = out.view(-1, channel, out_h, out_w)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        kernel, grad_kernel = ctx.saved_tensors\n",
    "\n",
    "        grad_input = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = UpFirDn2dBackward.apply(\n",
    "                grad_output,\n",
    "                kernel,\n",
    "                grad_kernel,\n",
    "                ctx.up,\n",
    "                ctx.down,\n",
    "                ctx.pad,\n",
    "                ctx.g_pad,\n",
    "                ctx.in_size,\n",
    "                ctx.out_size,\n",
    "            )\n",
    "\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n",
    "    if not isinstance(up, abc.Iterable):\n",
    "        up = (up, up)\n",
    "\n",
    "    if not isinstance(down, abc.Iterable):\n",
    "        down = (down, down)\n",
    "\n",
    "    if len(pad) == 2:\n",
    "        pad = (pad[0], pad[1], pad[0], pad[1])\n",
    "\n",
    "    if input.device.type == \"cpu\":\n",
    "        out = upfirdn2d_native(input, kernel, *up, *down, *pad)\n",
    "\n",
    "    else:\n",
    "        out = UpFirDn2d.apply(input, kernel, up, down, pad)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def upfirdn2d_native(\n",
    "    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n",
    "):\n",
    "    _, channel, in_h, in_w = input.shape\n",
    "    input = input.reshape(-1, in_h, in_w, 1)\n",
    "\n",
    "    _, in_h, in_w, minor = input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    out = input.view(-1, in_h, 1, in_w, 1, minor)\n",
    "    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n",
    "    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n",
    "\n",
    "    out = F.pad(\n",
    "        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n",
    "    )\n",
    "    out = out[\n",
    "        :,\n",
    "        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n",
    "        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n",
    "        :,\n",
    "    ]\n",
    "\n",
    "    out = out.permute(0, 3, 1, 2)\n",
    "    out = out.reshape(\n",
    "        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n",
    "    )\n",
    "    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n",
    "    out = F.conv2d(out, w)\n",
    "    out = out.reshape(\n",
    "        -1,\n",
    "        minor,\n",
    "        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n",
    "        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n",
    "    )\n",
    "    out = out.permute(0, 2, 3, 1)\n",
    "    out = out[:, ::down_y, ::down_x, :]\n",
    "\n",
    "    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h + down_y) // down_y\n",
    "    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w + down_x) // down_x\n",
    "\n",
    "    return out.view(-1, channel, out_h, out_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:29:16.127782Z",
     "iopub.status.busy": "2022-05-29T19:29:16.127504Z",
     "iopub.status.idle": "2022-05-29T19:29:16.134605Z",
     "shell.execute_reply": "2022-05-29T19:29:16.133860Z",
     "shell.execute_reply.started": "2022-05-29T19:29:16.127752Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_download_encoder_command(file_id, file_name):\n",
    "    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    save_path = os.path.join(os.path.dirname(current_directory), ENCODER_CODE_DIR, \"pretrained_models\")\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
    "    return url\n",
    "\n",
    "\n",
    "ENCODER_DOWNLOAD_PATH = {\"id\": \"1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0\", \"name\": \"psp_ffhq_encode.pt\"}\n",
    "NORMALIZE_MEAN = 0.5\n",
    "NORMALIZE_STD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download encoder and decoder weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:29:17.432573Z",
     "iopub.status.busy": "2022-05-29T19:29:17.432304Z",
     "iopub.status.idle": "2022-05-29T19:29:25.819588Z",
     "shell.execute_reply": "2022-05-29T19:29:25.818637Z",
     "shell.execute_reply.started": "2022-05-29T19:29:17.432543Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(f'./{ENCODER_CODE_DIR}')\n",
    "download_command = get_download_encoder_command(file_id=ENCODER_DOWNLOAD_PATH[\"id\"], file_name=ENCODER_DOWNLOAD_PATH[\"name\"])\n",
    "!{download_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and test encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:29:35.128414Z",
     "iopub.status.busy": "2022-05-29T19:29:35.127686Z",
     "iopub.status.idle": "2022-05-29T19:30:36.458495Z",
     "shell.execute_reply": "2022-05-29T19:30:36.457478Z",
     "shell.execute_reply.started": "2022-05-29T19:29:35.128376Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.psp import pSp\n",
    "\n",
    "\n",
    "encoder_args = {\n",
    "    \"model_path\": \"pretrained_models/psp_ffhq_encode.pt\",\n",
    "    \"image_path\": \"notebooks/images/input_img.jpg\",\n",
    "    \"transform\": transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([NORMALIZE_MEAN]*3, [NORMALIZE_STD]*3)])\n",
    "}\n",
    "\n",
    "\n",
    "def denormalize(t:torch.Tensor):\n",
    "    return t * NORMALIZE_STD + NORMALIZE_MEAN\n",
    "\n",
    "\n",
    "def tensor_to_image(t:torch.Tensor) -> PIL.Image:\n",
    "    return PIL.Image.fromarray((denormalize(t).detach().cpu().clamp(0, 1).permute(1, 2, 0) * 255).numpy().astype(np.uint8))\n",
    "\n",
    "\n",
    "def run_encoder(inputs, net, device):\n",
    "    return net(inputs.to(device).float(), randomize_noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:30:36.460856Z",
     "iopub.status.busy": "2022-05-29T19:30:36.460565Z",
     "iopub.status.idle": "2022-05-29T19:30:43.873258Z",
     "shell.execute_reply": "2022-05-29T19:30:43.872404Z",
     "shell.execute_reply.started": "2022-05-29T19:30:36.460818Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_pt_path = encoder_args['model_path']\n",
    "\n",
    "if os.path.getsize(encoder_pt_path) < 1000000:\n",
    "    raise ValueError(\"Pretrained encoder model was unable to be downloaded correctly!\")\n",
    "    \n",
    "ckpt = torch.load(encoder_pt_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "\n",
    "# update the training options\n",
    "opts['checkpoint_path'] = encoder_pt_path\n",
    "if 'learn_in_w' not in opts:\n",
    "    opts['learn_in_w'] = False\n",
    "if 'output_size' not in opts:\n",
    "    opts['output_size'] = 1024\n",
    "opts['device'] = device\n",
    "\n",
    "opts = Namespace(**opts)\n",
    "net = pSp(opts)\n",
    "net.eval()\n",
    "net.to(device)\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:30:43.911874Z",
     "iopub.status.busy": "2022-05-29T19:30:43.910037Z",
     "iopub.status.idle": "2022-05-29T19:30:44.056735Z",
     "shell.execute_reply": "2022-05-29T19:30:44.055668Z",
     "shell.execute_reply.started": "2022-05-29T19:30:43.911839Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and show input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:30:57.152619Z",
     "iopub.status.busy": "2022-05-29T19:30:57.152352Z",
     "iopub.status.idle": "2022-05-29T19:30:57.527082Z",
     "shell.execute_reply": "2022-05-29T19:30:57.526444Z",
     "shell.execute_reply.started": "2022-05-29T19:30:57.152588Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_path = encoder_args[\"image_path\"]\n",
    "original_image = PIL.Image.open(image_path)\n",
    "print('Size = ', original_image.size)\n",
    "original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare image as Pytorch input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:30:57.633372Z",
     "iopub.status.busy": "2022-05-29T19:30:57.633155Z",
     "iopub.status.idle": "2022-05-29T19:30:57.654433Z",
     "shell.execute_reply": "2022-05-29T19:30:57.653656Z",
     "shell.execute_reply.started": "2022-05-29T19:30:57.633346Z"
    }
   },
   "outputs": [],
   "source": [
    "img_transforms = encoder_args['transform']\n",
    "image_tensor = img_transforms(original_image)\n",
    "image_tensor.shape, image_tensor.min(), image_tensor.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:30:58.440490Z",
     "iopub.status.busy": "2022-05-29T19:30:58.440228Z",
     "iopub.status.idle": "2022-05-29T19:31:04.546185Z",
     "shell.execute_reply": "2022-05-29T19:31:04.545496Z",
     "shell.execute_reply.started": "2022-05-29T19:30:58.440462Z"
    }
   },
   "outputs": [],
   "source": [
    "out_image_tensor = run_encoder(image_tensor.unsqueeze(0), net, device)\n",
    "tensor_to_image(out_image_tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install HuggingFace transformers library (needed for CLIP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:31:15.756668Z",
     "iopub.status.busy": "2022-05-29T19:31:15.756402Z",
     "iopub.status.idle": "2022-05-29T19:31:21.031612Z",
     "shell.execute_reply": "2022-05-29T19:31:21.030854Z",
     "shell.execute_reply.started": "2022-05-29T19:31:15.756639Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "\n",
    "def generate(latent):\n",
    "    out_img_t = net.decoder([latent], randomize_noise=False, input_is_latent=True)\n",
    "\n",
    "    \n",
    "def get_default_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class HyperParameters:\n",
    "    lr:float=1e-3\n",
    "    betas:Tuple[float, float]=(0.9, 0.999)\n",
    "    wd:float=0.\n",
    "    low_grad_discarded_pct:float=0.\n",
    "    \n",
    "\n",
    "class ClipLoss:\n",
    "    def __init__(self, target_text:str, negative_texts:List[str]=None, device=None):\n",
    "        if negative_texts is None:\n",
    "            negative_texts = []\n",
    "        if device is None:\n",
    "            device = get_default_device()\n",
    "\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval()\n",
    "        self.model.requires_grad_(False).to(device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        self.inputs = self.processor(text=[target_text] + negative_texts, images=None, return_tensors=\"pt\", padding=True).to(device)\n",
    "        self.inner_loss = self._single_text_loss if len(negative_texts) == 0 else self._multi_text_loss\n",
    "    \n",
    "    def _single_text_loss(self, clip_out):\n",
    "        return -clip_out.logits_per_image.mean()\n",
    "    \n",
    "    #def _multi_text_loss(self, clip_out):\n",
    "    #    # TODO: try refactor as CELoss with target 0\n",
    "    #    # The `target_text` is always at the first position, hence 0 index\n",
    "    #    return -F.log_softmax(clip_out.logits_per_image, dim=-1)[:, 0].mean()\n",
    "    \n",
    "    def _multi_text_loss(self, clip_out):\n",
    "        n_texts = clip_out.logits_per_image.shape[-1]\n",
    "        target_idx = 0\n",
    "        negative_idxs = list(range(target_idx)) + list(range(target_idx+1, n_texts))\n",
    "        loss_val = -clip_out.logits_per_image[:, target_idx] + clip_out.logits_per_image[:, negative_idxs].mean()\n",
    "        return loss_val.mean()\n",
    "    \n",
    "    def _normalize(self, img_t):\n",
    "        norm_mean = torch.tensor(self.processor.feature_extractor.image_mean, device=img_t.device)[:, None, None]\n",
    "        norm_std = torch.tensor(self.processor.feature_extractor.image_std, device=img_t.device)[:, None, None]   \n",
    "        return (img_t - norm_mean) / norm_std\n",
    "    \n",
    "    def __call__(self, img_t:torch.Tensor, *args):\n",
    "        # Prepare input for CLIP: denormalize using our encoder stats and normalize using CLIP stats\n",
    "        clip_in_img_t = F.interpolate(self._normalize(denormalize(img_t)), size=224)\n",
    "        assert clip_in_img_t.requires_grad\n",
    "        \n",
    "        clip_out = self.model(pixel_values=clip_in_img_t, **self.inputs)\n",
    "        loss = self.inner_loss(clip_out)\n",
    "        return loss, f'{loss:.2f}'\n",
    "    \n",
    "    \n",
    "class FeatureLoss:\n",
    "    def __init__(self, initial_img, inner_loss=None, device=None, denormalize=True, layers_idxs:List[int]=None):\n",
    "        self.initial_img = initial_img\n",
    "        self.inner_loss = inner_loss if inner_loss is not None else nn.MSELoss()\n",
    "        if device is None:\n",
    "            device = get_default_device()\n",
    "        # Not sure whether convnext_tiny is the best option\n",
    "        self.net = models.convnext_tiny(pretrained=True).eval()\n",
    "        self.net.requires_grad_(False)\n",
    "        self.net.to(device)\n",
    "            \n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.denormalize = denormalize\n",
    "        \n",
    "        self.hook_outputs = []    \n",
    "        max_layer_idx = len(self.net.features) - 1      \n",
    "        if layers_idxs is None:\n",
    "            layers_idxs = range(max_layer_idx + 1)\n",
    "        for i in layers_idxs:\n",
    "            assert 0 <= i <= max_layer_idx, f'layers_idxs must be between 0 and {max_layer_idx}'\n",
    "            self.net.features[i].register_forward_hook(self._hook_fn)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            self.initial_img_ftrs_by_layer = self._extract_ftrs(self.initial_img.unsqueeze(0).to(device))\n",
    "        \n",
    "    def _hook_fn(self, module_self, inp, out):\n",
    "        self.hook_outputs.append(out)\n",
    "        \n",
    "    def _normalize(self, x):\n",
    "        if self.denormalize:\n",
    "            x = denormalize(x)\n",
    "        return self.normalize(x)\n",
    "    \n",
    "    def _maybe_reshape(self, x):\n",
    "        expected_spatial_shape = self.initial_img.shape[-2:]\n",
    "        if x.shape[-2:] != expected_spatial_shape:\n",
    "            x = F.interpolate(x, size=(expected_spatial_shape))\n",
    "        return x\n",
    "        \n",
    "    def _extract_ftrs(self, x):\n",
    "        self.net.features(self._maybe_reshape(self._normalize(x)))\n",
    "        # The feature maps are stored in self.hook_outputs\n",
    "        # Clone hook_outputs\n",
    "        all_ftrs = list(self.hook_outputs)\n",
    "        self.hook_outputs.clear()\n",
    "        return all_ftrs\n",
    "        \n",
    "    def __call__(self, x, *args):\n",
    "        x_ftrs_by_layer = self._extract_ftrs(x)\n",
    "        losses = torch.stack([\n",
    "            self.inner_loss(x_ftrs, initial_img_ftrs)\n",
    "            for x_ftrs, initial_img_ftrs in zip(x_ftrs_by_layer, self.initial_img_ftrs_by_layer)\n",
    "        ])\n",
    "        loss = losses.sum()\n",
    "        return loss, f'{loss:.3f}'\n",
    "\n",
    "    \n",
    "class ComposedLoss:\n",
    "    def __init__(self, losses:List[Tuple[Callable, float]]):\n",
    "        self.losses = losses\n",
    "        \n",
    "    def __call__(self, x, *args):\n",
    "        #return torch.stack([w * l(x, *args) for l, w in self.losses]).sum()\n",
    "        losses_info = [(l.__class__.__name__, l(x, *args), w) for l, w in self.losses]\n",
    "        displayable_losses = {name.replace('Loss', ''): val for name, (_, val), _ in losses_info}\n",
    "        loss = torch.stack([w * val for _, (val, _), w in losses_info]).sum()\n",
    "        return loss, displayable_losses\n",
    "    \n",
    "    \n",
    "class DisplayResultsCallback:\n",
    "    def __init__(self, n_steps:int, display_interval=20):\n",
    "        self.display_interval = display_interval\n",
    "        n_images_to_display = math.ceil(n_steps / display_interval)\n",
    "        self.n_cols = 5\n",
    "        self.n_rows = math.ceil(n_images_to_display / self.n_cols)\n",
    "        self.fig, self.axs = plt.subplots(self.n_rows, self.n_cols, figsize=(5 * self.n_cols, 5 * self.n_rows))\n",
    "        for row in self.axs:\n",
    "            for axis in row:\n",
    "                axis.set_axis_off()\n",
    "        self.display_handle = None\n",
    "\n",
    "    def __call__(self, step_idx, loss_val, image_t):\n",
    "        if (step_idx+1) % self.display_interval == 0:\n",
    "            image = tensor_to_image(image_t[0].detach().cpu())\n",
    "            flattened_idx = (step_idx+1) // self.display_interval - 1\n",
    "            row_idx = flattened_idx // self.n_cols\n",
    "            col_idx = flattened_idx % self.n_cols\n",
    "            axis = self.axs[row_idx, col_idx]\n",
    "            axis.imshow(image)\n",
    "            axis.set_title(f'Step{step_idx},loss={loss_val}')\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            if self.display_handle is None:\n",
    "                self.display_handle = IPython.display.display(self.fig, display_id=True)\n",
    "            else:\n",
    "                self.display_handle.update(self.fig)\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.fig.clear()\n",
    "\n",
    "\n",
    "class GifRecorderCallback:\n",
    "    def __init__(self, n_steps:int, save_interval=5, frame_duration_ms=100, gif_filename=\"transformation.gif\"):\n",
    "        self.n_steps = n_steps\n",
    "        self.save_interval = save_interval\n",
    "        self.frame_duration_ms = frame_duration_ms\n",
    "        self.gif_filename = gif_filename\n",
    "        self.temp_dir = tempfile.TemporaryDirectory()\n",
    "    \n",
    "    def _get_img_path(self, idx):\n",
    "        filename = f'img{idx}.jpg'\n",
    "        path = f\"{self.temp_dir.name}/{filename}\"\n",
    "        return path\n",
    "    \n",
    "    def _save_gif(self):\n",
    "        frames = [PIL.Image.open(self._get_img_path(i)) for i in range(1, (self.n_steps // self.save_interval) + 1)]\n",
    "        if len(frames) == 0: return\n",
    "        \n",
    "        frames[0].save(\n",
    "            self.gif_filename, \n",
    "            format=\"GIF\", \n",
    "            append_images=frames[1:], \n",
    "            save_all=True, \n",
    "            duration=self.frame_duration_ms, \n",
    "            loop=0\n",
    "        )\n",
    "        self.temp_dir.cleanup()\n",
    "\n",
    "    def __call__(self, step_idx, loss_val, image_t):\n",
    "        if (step_idx+1) % self.save_interval == 0:\n",
    "            img_idx = (step_idx+1) // self.save_interval\n",
    "            image = tensor_to_image(image_t[0].detach().cpu())\n",
    "            image.save(self._get_img_path(img_idx))\n",
    "            \n",
    "        if step_idx == (self.n_steps - 1):\n",
    "            self._save_gif()   \n",
    "\n",
    "    \n",
    "def unravel_index(indices, shape):\n",
    "    coefs = shape[1:].flipud().cumprod(dim=0).flipud()\n",
    "    coefs = torch.cat((coefs, coefs.new_tensor((1,))), dim=0)\n",
    "    coords = torch.div(indices[..., None], coefs, rounding_mode='trunc') % shape\n",
    "    return coords.t()\n",
    "    \n",
    "    \n",
    "def discard_grad_(x, bottom_grad_pct=0.2):\n",
    "    if bottom_grad_pct == 0: return\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        abs_grad = x.grad.abs().view(-1)\n",
    "        # TODO: should relative grad be better? (x.grad / x)\n",
    "        n_items_zeroed = int(bottom_grad_pct * x.grad.numel())\n",
    "        bottomk_indices = unravel_index(abs_grad.topk(n_items_zeroed, largest=False)[1], torch.tensor(x.shape, device=x.device))\n",
    "        x.grad[list(bottomk_indices)] = 0\n",
    "        \n",
    "\n",
    "def train(\n",
    "    n_steps:int, enc_dec:nn.Module, initial_img_t, target_text, negative_texts=None, hp:HyperParameters=None, device=None, \n",
    "    loss_fn:Callable=None, after_step:List[Callable]=None\n",
    "):\n",
    "    if hp is None:\n",
    "        hp = HyperParameters()\n",
    "    if device is None:\n",
    "        device = get_default_device()        \n",
    "    if loss_fn is None:\n",
    "        loss_fn = ClipLoss(target_text, negative_texts=negative_texts, device=device)\n",
    "    if after_step is None:\n",
    "        after_step = []\n",
    "\n",
    "    enc_dec.requires_grad_(False).eval().to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        #initial_latent = enc_dec.encoder(initial_img_t.unsqueeze(0).to(device).float(), randomize_noise=False)\n",
    "        _, initial_latent = enc_dec(\n",
    "            initial_img_t.unsqueeze(0).to(device).float(), randomize_noise=False, return_latents=True\n",
    "        )\n",
    "        \n",
    "    # shape (1, 18, 512)\n",
    "    latent = initial_latent.detach().clone()\n",
    "    latent.requires_grad_(True)\n",
    "    opt = torch.optim.Adam([latent], lr=hp.lr, betas=hp.betas, weight_decay=hp.wd)\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        img_t, _ = enc_dec.decoder([latent], randomize_noise=False, input_is_latent=True)\n",
    "        \n",
    "        loss, loss_to_display = loss_fn(img_t, latent, initial_latent)\n",
    "        loss.backward()  \n",
    "        discard_grad_(latent, hp.low_grad_discarded_pct)\n",
    "        opt.step()\n",
    "        \n",
    "        for cb in after_step: cb(i, loss_to_display, img_t)\n",
    "            \n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Install Kaggle and download FFHQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's only needed if you wish to use images from FFHQ as starting images, like some examples do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload token `kaggle.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy token to the directory that Kaggle expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images used in the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download arnaud58/flickrfaceshq-dataset-ffhq -f 00001.png -p ffhq --unzip\n",
    "!kaggle datasets download arnaud58/flickrfaceshq-dataset-ffhq -f 00011.png -p ffhq --unzip\n",
    "!kaggle datasets download arnaud58/flickrfaceshq-dataset-ffhq -f 00012.png -p ffhq --unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the full dataset, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download arnaud58/flickrfaceshq-dataset-ffhq --unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:31:26.222337Z",
     "iopub.status.busy": "2022-05-29T19:31:26.222078Z",
     "iopub.status.idle": "2022-05-29T19:31:26.397922Z",
     "shell.execute_reply": "2022-05-29T19:31:26.397244Z",
     "shell.execute_reply.started": "2022-05-29T19:31:26.222309Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/a/a0/Bill_Gates_2018.jpg\n",
    "initial_image_path = './Bill_Gates_2018.jpg'\n",
    "initial_image = PIL.Image.open(initial_image_path).crop((70, 40, 630, 600))\n",
    "print('Size = ', initial_image.size)\n",
    "initial_image_tensor = img_transforms(initial_image)\n",
    "initial_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align input image (optional, skip if the input image is already centered):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare image as Pytorch input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:31:28.870586Z",
     "iopub.status.busy": "2022-05-29T19:31:28.868085Z",
     "iopub.status.idle": "2022-05-29T19:31:28.887035Z",
     "shell.execute_reply": "2022-05-29T19:31:28.885867Z",
     "shell.execute_reply.started": "2022-05-29T19:31:28.870545Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_image_tensor = img_transforms(initial_image)\n",
    "initial_image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` parameters:\n",
    "- `n_steps`: number of updates/optimizer steps.\n",
    "- `enc_dec`: pSp network (encoder + StyleGAN 2 decoder). You should just pass `net` always.\n",
    "- `initial_image_tensor`: pyTorch tensor of shape (3, 256, 256) normalized with `NORMALIZE_MEAN` and `NORMALIZE_STD`. The conversion from a PIL image can be performed by `img_transforms`, as shown above.\n",
    "- `target_text`: the text that describes the face you want to get after several updates, starting from `initial_image_tensor`.\n",
    "- `negative_texts`: list of strings that shouldn't be a good description of the desired target image.\n",
    "- `hp`: hyperparameters\n",
    "  - `lr`, `betas`, `wd`: hyperparameters of Adam optimizer.\n",
    "  - `low_grad_discarded_pct`: the items of the gradient vector whose absolute value is one of the `low_grad_discarded_pct * 100`% smallest are discarded (zeroed) before every optimizer step. 0 means all elements of the latent vector are updated, and 1 means none, which wouldn't make sense as you'd get the original image. It's meant to reduce entanglement but it may not be the best option.\n",
    "- `device`: specified with PyTorch format.\n",
    "- `loss_fn`: loss function, a callable that must return a scalar tensor. Parameters:\n",
    "  - img_t: image tensor generated from the current `latent`.\n",
    "  - latent: current latent vector.\n",
    "  - initial_latent: latent vector that makes StyleGAN generate `initial_image_tensor`.\n",
    "- `after_step`: callable invoked after every optimizer step. Parameters:\n",
    "  - step_idx: index of last step.\n",
    "  - loss_val: last output of the loss function.\n",
    "  - img_t: image tensor generated from the current `latent`.\n",
    "  \n",
    "Other variables to configure:\n",
    "- `FEATURE_LOSS_W`: weight that multiplies the feature loss. Set to 0 to disable the feature loss. The higher its value, the more the generated images will resemble the initial image.\n",
    "- `save_gif`: if `True`, a gif of the transformation is stored in `./transformation.gif`\n",
    "\n",
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T19:31:41.004586Z",
     "iopub.status.busy": "2022-05-29T19:31:41.004319Z",
     "iopub.status.idle": "2022-05-29T19:32:49.662911Z",
     "shell.execute_reply": "2022-05-29T19:32:49.662116Z",
     "shell.execute_reply.started": "2022-05-29T19:31:41.004556Z"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "target_text = \"Mark Zuckerberg\"\n",
    "negative_texts = []\n",
    "clip_loss = ClipLoss(target_text, negative_texts=negative_texts, device=device)\n",
    "FEATURE_LOSS_W = 0\n",
    "save_gif = True\n",
    "gif_filename = \"transformation.gif\"\n",
    "\n",
    "if FEATURE_LOSS_W > 0:\n",
    "    feature_loss = FeatureLoss(\n",
    "        initial_image_tensor, \n",
    "        device=device, \n",
    "    )\n",
    "    loss_fn = ComposedLoss([\n",
    "        (clip_loss, 1.),\n",
    "        (feature_loss, FEATURE_LOSS_W)\n",
    "    ])\n",
    "else:\n",
    "    loss_fn = clip_loss\n",
    "    \n",
    "callbacks = [DisplayResultsCallback(n_steps, display_interval=5)]\n",
    "if save_gif:\n",
    "    callbacks.append(GifRecorderCallback(\n",
    "        n_steps, save_interval=5, frame_duration_ms=100, gif_filename=gif_filename,\n",
    "    ))\n",
    "\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    target_text,\n",
    "    negative_texts=negative_texts,\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0.),\n",
    "    after_step=callbacks,\n",
    "    loss_fn=loss_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the gif if you have set `save_gif=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.FileLink(gif_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An old woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image_path = 'ffhq/00001.png'\n",
    "initial_image = PIL.Image.open(initial_image_path)\n",
    "print('Size = ', initial_image.size)\n",
    "initial_image_tensor = img_transforms(initial_image)\n",
    "initial_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"An old woman\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An old man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"An old man\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nami, One Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"Nami, One Piece\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pink-haired woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A pink-haired woman\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pink-haired man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A pink-haired man\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A woman with big eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A woman with big eyes\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With feature loss, the result is more similar to the initial image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "target_text = \"A woman with big eyes\"\n",
    "negative_texts = []\n",
    "feature_loss = FeatureLoss(\n",
    "    initial_image_tensor, \n",
    "    device=device, \n",
    ")\n",
    "FEATURE_LOSS_W = 100.\n",
    "train(\n",
    "    n_steps, net, initial_image_tensor, target_text,\n",
    "    negative_texts=negative_texts,\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)],\n",
    "    loss_fn=ComposedLoss([\n",
    "        (ClipLoss(target_text, negative_texts=negative_texts, device=device), 1.),\n",
    "        (feature_loss, FEATURE_LOSS_W)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A woman that didn't apply sun screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A woman that didn't apply sun screen\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without blonde hair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A woman that didn't apply sun screen\",\n",
    "    negative_texts=[\"Blond hair\"],\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rihanna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"Rihanna\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andrew Ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image_path = 'ffhq/00011.png'\n",
    "initial_image = PIL.Image.open(initial_image_path)\n",
    "print('Size = ', initial_image.size)\n",
    "initial_image_tensor = img_transforms(initial_image)\n",
    "initial_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"Andrew Ng\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Draculesque man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A Draculesque man\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A white-haired young man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image_path = 'ffhq/00012.png'\n",
    "initial_image = PIL.Image.open(initial_image_path)\n",
    "print('Size = ', initial_image.size)\n",
    "initial_image_tensor = img_transforms(initial_image)\n",
    "initial_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"A white-haired young man\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bill Gates -> Elon Musk interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/a/a0/Bill_Gates_2018.jpg\n",
    "initial_image_path = './Bill_Gates_2018.jpg'\n",
    "initial_image = PIL.Image.open(initial_image_path).crop((70, 40, 630, 600))\n",
    "print('Size = ', initial_image.size)\n",
    "initial_image_tensor = img_transforms(initial_image)\n",
    "initial_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"Elon Musk\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bill Gates -> Mark Zuckerberg interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "train(\n",
    "    n_steps, \n",
    "    net, \n",
    "    initial_image_tensor, \n",
    "    \"Mark Zuckerberg\",\n",
    "    hp=HyperParameters(lr=1e-2, low_grad_discarded_pct=0),\n",
    "    after_step=[DisplayResultsCallback(n_steps, display_interval=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
